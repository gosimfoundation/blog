---
title: 模型训练最重要的依然是 Scaling | Open AGI Forum
date: 24 June 2025
description: '对话阿里通义千问 Qwen 多语言负责人杨宝嵩'
image: /blogs-img/blog12-杨宝嵩.jpg
alt: First Post
ogImage: /blogs-img/blog12-杨宝嵩.jpg
tags: ['gosim', 'open-source', 'grassroots']
published: true
---

*作者 & 采访 | 唐小引 Echo Tang*
*编辑 | 王诗琪、何苗*
*出品 | GOSIM 开源创新汇*

近年来，随着大模型技术在全球范围内快速崛起，“出海”成为每家 AI 公司绕不开的话题。在这场激烈的全球竞逐中，阿里巴巴通义实验室研发的通义千问（Qwen）表现尤为抢眼。**作为全球开源社区 Hugging Face 上下载量最高的模型之一，Qwen 甚至在欧美用户中收获了超越中文用户的热情拥趸**。这种国际化成功的背后，是通义实验室在多语言技术上的深度布局与战略眼光。

在法国巴黎举行的 GOSIM AI Paris 大会现场，**CSDN&《新程序员》执行总编唐小引**与**阿里巴巴通义实验室研究科学家杨宝嵩**进行了深入交流。作为 Qwen 多语言能力的负责人，杨宝嵩带领团队赋予了 Qwen 模型备受称赞的支持全球 119 种语言的能力。采访中，他透露了 Qwen 一开始就将国际化视作核心战略，优先考虑多语言数据优化，确保全球用户都能公平享受 AI 带来的技术普惠。

杨宝嵩指出，大模型出海并非简单的语言翻译问题，而是要直面不同语言背后复杂的文化规范和禁忌内容。在多语言安全性与文化对齐方面，团队专门建立了一整套复杂的文化标注体系，覆盖上千个细粒度分类，以确保模型内容在全球各地区都安全、合规、有效。这项工作耗费巨大，但杨宝嵩认为意义非凡。

谈到多语言大模型技术的挑战时，他特别指出当前业界普遍存在的“多语言推理难题”。大模型在处理非母语用户提问时，常常会出现混杂多种语言的情况，这一问题目前尚无模型能够完美解决。对此，通义团队采取了折中的策略：在擅长的语言上用原生语言进行推理，而对于低资源的小语种则统一采用英语进行内部推理，力求模型输出更稳定与一致。

此次专访也触及了当前热门的 Scaling Law（规模定律）问题。杨宝嵩认为，未来持续扩大模型规模和数据量仍然至关重要。但在算力和数据面临瓶颈的现实情况下，提升数据“知识密度”与利用高质量合成数据将成为新的突破口。正因如此，Qwen 团队正积极尝试利用模型生成的数据，持续提升训练数据质量，以应对规模扩张的瓶颈期。

**以下为对话实录：**

**唐小引**：大家好，欢迎收看由 GOSIM 主办的 Open AGI Forum 栏目，我是来自 CSDN 的唐小引，在法国巴黎举行的 GOSIM AI Paris 现场，我们很荣幸地邀请到了阿里巴巴通义实验室研究科学家杨宝嵩老师，和大家一起分享自己的程序人生，以及大家非常关心的通义千问（Qwen）的一些故事和进展。欢迎杨老师，先请您和大家打个招呼并做一下自我介绍。

**杨宝嵩**：大家好，我是杨宝嵩，来自阿里巴巴通义实验室。目前主要负责千问及千问系列模型的多语言能力研发，包括机器翻译能力等。

**唐小引：我先讲个小插曲，“千问”这个模型英文名“Qwen”到底该怎么读还有不少的讨论，在海外有谐音“Queen”（女王）、“Q wen”的不同发音，能不能从官方角度给我们讲一讲？**

**杨宝嵩**：其实我觉得大家喜欢怎么叫就怎么叫吧，不过官方的读法应该就是念作 “Queen”。当然，“Q wen”也挺好听的。国内的话，大家还是叫“千问”最习惯、最顺口。

*作者注：英语中的 "Q" 通常发 /k/ 的音，并与后面的 "w" 结合成 /kw/ 的音，就像 “queen”（女王）或“quick”（快的）一样。国内的发音会比较类似“kun”，而对于不熟悉中文拼音的英语母语者来说，他们则会按照英语的自然拼读规则来读，“queen”的谐音就由此而来，也使得不少 AI 开发者一度调侃 Llama 是 King，Qwen 是 Queen。*

**唐小引：此次来到巴黎参加 GOSIM AI Paris 感受如何？有没有什么特别的收获或启发可以和大家分享？**

**杨宝嵩**：首先，我觉得巴黎是一个非常漂亮的地方，有深厚的历史文化底蕴，食物也很棒。这次大会在巴黎举办，让我们有机会见到了很多从事开源工作的同行，包括社区里的许多人，比如 Hugging Face 的工程师们。基本上各大开源组织这次都来了。也见到了很多新生代的朋友，有些还在读硕士、博士，比如像 Llama Factory 的作者等，这次大会真的是一个很好的交流契机。在国内大家平时都在埋头做研究，其实很少有机会和全球同行面对面深入交流。

**唐小引：千问在 Hugging Face 上的下载量非常高，在现场有和 Hugging Face 的工程师交流吗？**

**杨宝嵩**：对，Hugging Face 的工程师们确实很认可我们国内的开源模型，特别是千问。现在千问在 Hugging Face 上的下载量确实非常大，据我了解，基于千问衍生出来的模型数已经超过十万个了。

**唐小引：没错，我们之前统计过，无论大小模型，千问的下载量都遥遥领先。而且我知道，千问在北美（尤其硅谷）影响力很高，在 X 平台上有很多海外关注者。这次千问来到欧洲的巴黎，您能不能谈谈千问在中国、欧美的情况？**

**杨宝嵩**：前不久我们开放了 Qwen Chat，发现用户量最大的其实是英语社区，中文用户反而并不是排名第二大的。我记得中文好像只排到第三或第四。当时看到这个数据也挺有意思的——这证明千问在整个国际社区中的影响力还是非常大的。

**唐小引：作为一个起于中国的模型，中文用户量居然排不到第二，这是为什么？和你们的策略有关吗？**

**杨宝嵩**：对，这和我们的策略直接相关。从千问项目启动之初，国际化就是我们的重要目标之一。我们希望千问能服务全球用户，而不仅仅是中国用户。所以在模型训练的一开始，我们就特别重视让模型具备多语言能力，面向全球市场去优化。具体来说，在国内做大模型时，我们团队很早就开始往模型里加入多种语言的数据，训练一个多语言的基础模型。即使这么做会牺牲掉一些中英文、代码或者数学推理方面的性能，我们仍然选择优先保证模型的多语言效果。这相当于为模型定下了一个调子：千问不是只为中英文优化的，它生来就是面向全球、多语言的。

**唐小引**：一直以来，Qwen 的多语言能力都是由您来负责与主导，这是由于和您个人的求学背景、从事的 NLP 机器翻译方向密切相关吗？

**杨宝嵩**：没错。我大概 2013 年开始踏入机器翻译领域，当时深度学习才刚刚兴起，AI 圈子还没现在这么热。但即使如此，那个时候选择读博投入 AI 还是需要点魄力的，我们实验室当时很多师兄弟最后都毕不了业。一直到 2015、2016 年左右，深度学习的浪潮真正起来之后，我们 AI 这一行一下子活跃了起来，可以说迎来了最好的发展时机。

**唐小引**：对，神经网络和深度学习那时迎来大爆发。

**杨宝嵩**：是的。我个人从那时候起一直坚持的研究兴趣就是多语言和翻译方向。即使到现在，我依然在专注做机器翻译相关的研究工作。之所以对多语言情有独钟，是因为我始终觉得对于 AI 来说，或者说对于“技术普惠”来说，多语言是最重要也最值得做的一件事。我们不希望未来世界上只有中美两国能够享受 AI 带来的福利。所以从 AI 平权、让技术惠及更多不同语言和文化背景的人这个角度，多语言能力是必须要做好的一件事情。这是第一点，我们把格局放大来看。第二点，不管是中国的企业出海，还是各种业务的全球化，多语言翻译这些技术都是必须长期投入、长期攻坚的能力。

第三点也和我个人兴趣有关，我确实觉得这个方向很有意思。在加入阿里之前，我就一直有一个期望：希望在我的职业生涯里能够实现这样的场景——戴上一副智能眼镜或者头显出门，所有听到的外语都能实时翻译成自己听得懂的内容，所有看到的文字都自动转换成你能读懂的语言。如果是两三年前甚至五年前来看，这种想法还是非常遥远的。但大模型出现之后，随着大模型和视觉、视频生成、语音识别、TTS（语音合成）这些能力逐步融合到一起，现在已经有非常多的 AI 厂商或者 AI 外设公司开始在做这件事了。我相信这个目标的实现速度会越来越快。

**唐小引：千问多语言能力的成功离不开您的长期积累。但在追求卓越多语言能力的过程中，肯定会遇到各种挑战。能否请您谈一谈，在打造千问的多语言能力时，您遇到了哪些独特的挑战？比如，不同语言有各自复杂的文化背景和内容规范，这些在模型训练和优化时是如何应对的？**

**杨宝嵩**：做多语言大模型时，我们确实遇到过一些非常棘手的挑战。我从两方面来说：首先是数据质量和标注标准的问题。对于一些高资源的主流语言，比如英文、中文，我们有大量数据可以用，模型也容易学到这些语言里的规律和规范。但对于许多小语种来说，可用的数据很少，而且不同语言、不同文化对于内容的禁忌和敏感点也不一样。

举个例子，在英文或中文的数据中过滤有害内容也许相对简单，比如涉黄、涉政信息，有比较明确的标准可以参考，模型训练时可以通过已有的内容安全模型来识别这些不良信息。但是换成一些其他文化背景的语言就复杂得多。比如阿拉伯语世界，不同国家、不同宗教派别有各自忌讳的内容。某些在一个文化里正常的信息，在另一个文化语境里可能就是禁忌，甚至严重冒犯。那对于这些语言的数据，我们如果想做好内容安全和价值观对齐，就需要付出额外的努力去标注和区分。有些规则可以从英文、中文这些大语种中“迁移学习”过来，用统一标准去过滤。但也有一些跟当地文化习俗密切相关的内容，需要我们专门去理解和处理。我们团队曾经为多语言内容安全建立过一整套庞大的文化标注体系，涵盖了可能上千个类目的细粒度标签，就是为了针对不同语言、不同文化背景下的数据做分类打标。

**唐小引**：这听起来需要投入巨大的人工标注和专业知识工作量。

**杨宝嵩**：是的，确实是一个非常庞杂的工程。这也算是模型出海、多语言化过程中必须啃的硬骨头吧。好在我们通过一些数据合成的方法，结合人工审核，逐渐建立起了适用于多语言的内容筛查规则和标注标准。这方面的投入虽然大，但我觉得非常值得。如果不做好不同文化的价值观和禁忌对齐，模型在一些小语种场景下可能就无法真正落地。

**唐小引**：千问前段时间发布了 Qwen3，我们编辑部也是连夜关注了更新动态。作为千问多语言能力的负责人，在最新版本研发过程中，有没有什么最让您头疼但最终解决（或有所突破）的问题？

**杨宝嵩**：千问 3.0 在多语言方面其实每一个问题都挺让人头疼的。要说其中一个挑战，我觉得可以讲多语言环境下模型推理过程用什么语言这个问题。现在大模型都在强调 “推理” 或 “思考” 的能力，就是 Chain-of-Thought（思维链，CoT）。我们最开始也希望模型在用什么语言被提问，就用相同语言去思考和推理。比如用户用日语问问题，我们理想情况下希望模型内部也是用日语来推理和形成答案，那最后输出当然也是日语。这听起来很自然，对吧？

**唐小引**：按常理这是应该做到的。

**杨宝嵩**：是的，听上去是正常逻辑。但实际上我们发现，目前市面上几乎所有的大模型在这方面都做不到完美。一些模型在启用了思维链后，它的思考过程会混杂多种语言——可能这句话还在用英文想，下一句又蹦出中文或者其它语言。对于用户来说看到这样的思维过程是很困惑的。如果模型能力不够强的话，更极端的情况是思考过程反复来回，用不同语言絮絮叨叨停不下来，导致既浪费算力 token，又影响最后效果。

我们在做千问 3 的时候专门考虑了这个问题，尝试让模型严格用提问对应的语言进行推理。但后来发现，以当前的技术水平，一些低资源语言（模型不太擅长的语言）的生成和推理能力确实跟不上。强行让它用那种语言去完整推理，反而可能出 Bug（比如思维链卡壳、循环等问题），更影响最终答案质量。

权衡之下，我们目前采用的策略是折中方案：至少保证最后给出的回答（Answer）用用户提问的语言，而隐藏的思维链部分，对于模型特长的语种（模型在那些语言上能力强），我们尽量也用对应语言去做推理；但对于那些模型暂时不太擅长的小语种，我们就干脆统一用一种通用语言（比如英语）来进行思考过程。这至少避免了思维链里多语言夹杂的问题，用单一语言思考总比混杂好多种语言要好，让推理过程干净一致一些。

**唐小引**：您这么一说，我深有同感。我自己在使用一些大模型（比如 Claude）时也遇到过类似情况：我用中文提问，它最后回答我是中文，但中间展示的思考过程蹦出了英文，感觉这是大家普遍面临的问题。那现在有模型完全解决了这个问题吗？

**杨宝嵩**：老实说，目前还没有看到哪个模型做到了这点。而且我觉得这个问题恐怕短期内也很难彻底解决。对于高资源的大语种来说，可能比较容易一些，但涉及到上百种语言让每一种都用自己的语言进行复杂推理，难度太大了。小语种本身模型掌握的生成能力就弱，推理能力也弱，即使让它借助英文等强语言来迁移，最后还是需要用小语种输出复杂逻辑。这种情况下，要模型思维链 100%使用对应的小语种，确实是一个极具挑战性的方向。

**唐小引**：明白，这也算是多语言大模型推理的下一步难关吧。那回到思维链本身，我想接着问一个相关的问题。我们知道有些大模型可以把推理过程显示出来，就是所谓“思维链”。最开始很多人看到模型能展示一步步的思考过程，觉得非常惊艳。但后来大家也发现一个现实问题：这些思维链会耗费大量 token，也就是算力和响应长度。对于这个问题，你们在模型设计上有考虑过吗？有没有什么改进思路？

**杨宝嵩**：这个问题现在是大家普遍关注的一个点，我们当然考虑到了。在最新的 Qwen3 模型里，我们做了一些针对性的改进。具体来说，我们融合了两种对话模式：一种是开启“思考”的模式，另一种是安静地不给出冗长思考过程的模式。开发者或者用户可以选择开关这个“思维链”输出。此外我们还加入了思考过程长度的控制，也就是可以限定它内部思考最多展开多少步，以防止思维链过长变成絮絮叨叨。目前学术界也有很多关于“思维效率”（Efficiency of Thinking）的研究热点，大家都在探索如何用更短的思考过程达到相同甚至更好的效果，或者根据任务自动判断需不需要显式展开思考。我相信这个问题很快会有所突破，因为实在是太多人在研究优化它了，大家都卷进来之后，进展会很快。

**唐小引**：听您这么说，感觉接下来在这方面不远的将来会有改变？

**杨宝嵩**：我认为会的。因为我已经看到非常多在读博士、学术界人士，还有很多公司团队（包括我们自己）都在钻研这个问题。整个行业一旦卷起来，这个事情应该很快就能得到大幅改进。我猜很快你就不会再被无用的长思维链烦恼了。

**唐小引**：接下来我们聊聊 Scaling Law（规模定律） 和大小模型的问题。当今业界在训练大模型时，面临数据和算力上的天花板，很多人都在讨论经典的 Scaling Law 是否还能持续有效。您在 GOSIM 的演讲中，提到了一个概念：“知识密度”。现在业内也有人讨论，模型能力的提升是否可以通过提升训练数据的知识密度来实现。您是否可以先跟我们讲解一下知识密度在大模型中的作用？

**杨宝嵩**：知识密度简单理解就是单位数据中蕴含有用知识的浓度。我认为这是一个非常重要的研究方向。这半年多来业界在这方面其实已经有很直观的体现：你会发现现在参数规模只有 4B 左右的小模型，在很多任务上的效果，可能已经赶上甚至超过半年前、一年前那些 70B 级别大模型的效果。这其中很大程度的收益就来自于我们给模型喂的训练数据的知识密度和质量提升了。

过去模型训练可能侧重拼数据量、拼参数规模，现在更多人开始关注数据的精细打磨——过滤掉噪音和冗余，压缩知识点，提高有效信息含量。结果就是模型用更少的数据也能学到同样甚至更多的东西。所以我觉得提高知识密度本身肯定是大家会持续研究下去的一个技术方向。另一方面，知识密度提升还有一个直接好处是训练效率提高了。可能原来达到某个指标需要喂给模型 10 万亿词的语料，现在也许 1 万亿或 2 万亿就够了。这对于节省算力、降低训练成本也很重要，这是目前很令人惊喜的一个趋势。

**唐小引**：提升知识密度现在算是大家训练模型时攻关的主要方向之一吗？

**杨宝嵩**：我觉得至少是主要方向之一，但未必是唯一最重要的方向。

**唐小引：那当前最重要的方向是什么呢？或者说，在提升模型能力方面，排在第一位的突破点是什么？**

**杨宝嵩**：如果从基础模型（Base Model）的研究角度来看，我个人认为仍然是 Scaling。也就是说，让模型规模继续变大、见到更多的数据，这件事本身还是要持续去探索的。包括现在很多人在研究的数据合成，其实某种程度上也是为了服务于 Scaling，可以看作是用另一种方式继续“喂”模型更多有效数据。所以我心目中排在第一位的依然是扩展模型规模和数据规模（Scaling Law 的延续）。

**唐小引**：那么在目前算力和原始数据受限的情况下，从您做多语言模型的经验来看，有哪些办法可以帮助我们让 Scaling Law 得以延续？很多人都说，合成数据可能是当前非常有效的一个路径。您在这方面的思考是怎样的？

**杨宝嵩**：合成数据我理解现在主要有两大方向。第一类方向，是用模型去“创造”新的知识和内容。这是大家最近很关注的一种做法。例如，我们可以让模型自己生成一些高质量的问题再配上答案，或者基于最新的新闻事件自动总结出一些知识点和推论，用于训练模型的推理能力等等。简单说，就是利用模型已有的知识去编造或推演出新的知识来训练模型。这听起来有点自我进化的味道，但如果做好了，确实可以进一步提升模型的能力上限，相当于突破原始数据没有提供的新知识。这是一个很有前景的研究方向。

第二类相对成熟一点，就是提升知识密度相关的合成方法。比如，我们可以让模型把冗余的数据压缩一下，提炼出核心内容，或者对重复的、没价值的数据进行剪裁过滤，使训练语料“更干净”。这一类做法直接解决的就是前面提到的知识密度问题。通过模型生成或筛选，让训练数据的信息含量更高。总的来说，我觉得数据合成在未来一定是一个非常重要的方向，不管是创造新知识，还是提高知识密度，对延续 Scaling Law 都非常关键。

**唐小引**：的确，现在只要谈到大模型训练，大家几乎都会提“要不要造数据”。在 NVIDIA GTC 大会上，黄仁勋也特地强调了合成数据对于继续推动 Scaling Law 的作用。不过合成数据可能带来另一个问题：如果所有人都用模型生成数据来训练模型，那么这些模型产出的内容又会被重新爬到互联网上，进入下一个循环的训练数据里。长此以往，训练语料中会混入越来越多 AI 自己生成的东西。这样一来，模型团队在训练时需不需要识别、剔除那些合成的数据呢？会不会出现“模型吃自己的输出”导致知识面越来越窄的问题？

**杨宝嵩**：这个现象可以理解为一种“数据回流”问题，确实值得关注。但我的观点是，首先合成数据这件事历史上一直存在，并不是有了大模型以后才突然出现的新问题。举个例子，在 ChatGPT 诞生以前，其实互联网上早就充斥着大量模板生成的内容和机器翻译的文本数据。这些严格来说也是“合成”的。我们的模型以前训练也或多或少用了这些数据。所以训练数据里混入机器生成内容的问题早就有，只是以前大家没太当回事。现在大模型写文章比普通人都像模像样了，大家反而开始担心它填充了我们的训练语料。

**唐小引**：听您这么一说，我也发现一个有趣的现象：虽然现在大模型很热，但真正往深里看，其实没有凭空出现多少“全新的问题”。您看，像强化学习那些概念早在 DeepSeek 带火之后发展了好多年了；合成数据、幻觉这些话题以前也一直存在。大家好像都在卷大模型新赛道，但底层的技术积累和要解决的问题，却仿佛是这些年来一直延续的老问题。这是为什么呢？

**杨宝嵩**：我觉得这说明很多技术或者问题本身并不是突然冒出来的，而是由来已久、水到渠成的。就拿神经网络来说，这个概念都提出了几十年了，不也是最近十年才因为算力和数据的突破而大放异彩吗？以前技术条件不成熟，大家摸索了很久。但一旦时机成熟，老技术也能焕发新活力。所以现在大模型看似新奇，其实背后很多原理和挑战还是之前的，只不过规模变大了，场景变广了而已。

**唐小引**：的确，AI 行业也是几起几落起伏发展，神经网络曾经也坐了几十年的冷板凳才等到今天的热潮。面对海量合成数据回流，模型训练该如何应对？

**杨宝嵩**：问题的第一面是，合成数据无可避免，会混在训练数据里，这是现实，没有必要过度恐慌。第二面是，我们也注意到，大模型生成的很多内容质量并不差，有时候甚至比人写的还好。那么对我们而言，真正需要过滤和警惕的是什么？无非就是错误的信息，或者模型的幻觉产出（瞎编乱造的东西）。除此以外，如果模型生成了一段内容正确、表述清晰的文字，从利用角度看，其实并没有什么不可以用的。当然，这里有个度的问题，不可能我们训练数据全都是模型自己写的，那样风格和角度都会单一化，对模型长远发展不利。我之前在做多语言训练时就观察到：人类使用语言有非常丰富的多样性，模型如果老吃自己产出的东西，难免会风格趋同、内容单调，长远看对语言多样性和创新是不好的。

所以我们在利用合成数据时也会注意比例，不会让它淹没人类数据。同时该过滤的仍然要过滤，比如模型内容里的错误和谬误，我们一定会想办法去识别和剔除。好在现在也有一些技术手段可以检测一段话是模型写的还是人写的。说到底，大模型生成的内容还是有一些可以识别的特征的。当然模型越来越强，那特征可能会越来越弱，但至少目前来看，这方面研究也在进行。

**唐小引：只要合成数据经过你们正常的数据清洗、质量评估，达标了，就可以拿来用，但同时也会控制一个使用比例？**

**杨宝嵩**：没错，我们并不排斥模型生成的数据，只要它质量够高，我们就把它当普通训练数据用就好了。当然也不会无节制地全用，会混合比例，保证多样性。现在也有一些研究工作尝试在训练时显式打标签，比如给合成数据前面加个特殊标记，让模型知道哪些是 AI 生成、哪些是人类数据，再一块训练。最近确实有一些论文在探讨这种做法，但我个人觉得没特别必要一定要做显式区分。完全可以在预训练的不同时期用不同的数据源来训练，这样达到的效果其实也差不多。所以总的来说，我们不会太纠结某条数据是不是模型写的，更关注它是不是真实、正确、丰富多样的。

**唐小引**：接下来聊聊大小模型的问题。业界不同团队在模型规模路线上的选择不太一样：比如 DeepSeek 一直走的是超大参数模型路线，而像千问以及许多开源模型，基本上大小模型都有布局，近期也发布了好些小模型。您能谈谈千问在大小模型上的考虑吗？

**杨宝嵩**：这其实和每个机构或公司的战略方向有关。对于阿里来说，因为阿里云和通义实验室实际上是密切协同的关系，我们需要考虑云上客户的需求。阿里的很多客户业务场景是在终端或者边缘设备上跑的，他们对推理效率、时延有非常高的要求。这种情况下，如果我们的模型只能提供一个上百亿、上千亿参数的超大模型，显然无法满足很多落地需求。所以第一个考虑，我们肯定需要推出小模型，在保证一定能力的前提下尽量精简，方便部署到各种环境里。其次，从开源和科研的角度，我们也希望不要让模型规模成为创新的门槛。太大的模型，学术界的老师和同学们没法跑实验，普通开发者也用不起。甚至对我们自己来说，训练和微调一个几百亿参数的模型都已经非常困难了，更别提数千亿的。所以为了让社区生态发展得更繁荣，我们选择开放小尺寸和中等尺寸的模型，并且持续打磨提升它们的效果。让更多人可以在千问模型上做二次开发、验证新想法。这一点上我们最近也挺有成就感的——看到非常多论文、项目开始使用千问模型作为基座。

**唐小引**：是的，现在不少开源模型提到“基于某某大模型微调”时，都会有千问的名字出现。比如李飞飞团队的 s1 模型、Manus 据说也是基于 Claude 和千问。

**杨宝嵩**：这个我们也很高兴看到。

**唐小引：千问的小模型能够被这么多成果应用，也说明走“小而精”路线很有价值。您刚才也提到，很多小模型在某些能力上已经可以和大模型相当。这自然让人想到端侧部署的问题——既然小模型足够强，那把它们放到手机、耳机、眼镜这些设备上就成为可能。基于您对多语言和模型的研究，千问在端侧部署和 AI 外设上有怎样的规划和考虑？**

**杨宝嵩**：我们一直在思考布局。其实现在已经有很多厂商在做各种各样的 AI 外设（设备），比如智能眼镜、智能耳机，甚至一些家用电器等。这些设备由于算力、功耗限制，往往只能跑很小的模型或者本地推理的模型。它们所需要的功能有些也相对没那么复杂，比如耳机可能需要做语音识别或者简单的同声传译，这种场景下也许一个 4B 参数量级甚至更小的模型就能胜任。我们看到的趋势是，模型在终端侧的部署需求会越来越多。千问肯定也会顺应这个趋势，推出适合端侧的小模型版本，并针对这些应用场景进行优化。

**唐小引**：能具体举个 AI 眼镜的例子吗？比如 Meta 的智能眼镜，我当时很惊喜地看到号称结合了 Llama 模型的能力，所以立刻买来试了一下。结果不知什么原因，我尝试跟它语音交互的时候，感觉 Llama 并没有很好地在发挥作用。目前对我来说，它还只是一个时尚的穿戴设备。我对它的期待其实要高得多——比如我希望戴着它逛卢浮宫时，它能直接用我的语言给我讲解眼前蒙娜丽莎的故事；当我看到一幅陌生但有趣的画作时，它告诉我这是谁的作品，有什么典故和寓意；又比如在异国他乡，如果周围有人说我听不懂的语言，它能实时翻译告诉我对方在说什么……总之，我理想中的智能眼镜可以辅助生活工作的方方面面。但是现在来看，它的实际表现离我的期望还差得很远。

**杨宝嵩**：这些需求非常典型，而且很有代表性。我认为目前这种智能眼镜还处在一个起步的阶段。早期尝鲜总是要付出一点代价，智能手机刚出来的时候，第一批 iPhone 用户也很难想象十多年后手机会如此普及、功能如此强大。AI 外设现在的发展也类似，先从基础功能做起，随着软硬件不断成熟，再一步步逼近我们的理想目标。一方面，目前硬件层面也许还跟不上，比如电池、算力、网络连接这些都需要突破。另一方面，软件层面也在快速演进中。大模型的多模态融合、与外部世界交互（例如工具调用）现在都在积极探索，但确实还没有完全成熟。所以像您刚才提到的那些场景——多语言的语音翻译、跨语言的文字识别甚至即时生成展现——目前业内都还在攻关中，算是很有挑战的课题。

**唐小引**：您觉得以千问的能力，在现在这个时间点，AI 外设上可以实现到什么程度？您的目标又是希望达到怎样的效果呢？

**杨宝嵩**：就目前来看，我了解到的情况是，国内外许多车企、手机厂商以及智能硬件公司都在和阿里云通义团队合作，尝试把千问相关的模型能力嵌入他们的设备中。我们实验室发布过一个 Mobile-Agent 系统，虽然不是我们多语言组负责的，但也是通义实验室的成果。它相当于是一个基于视觉的智能代理，可以通过“看”来操作手机。比如你对手机说“帮我点一份外卖”，Mobile-Agent 就能模拟人在屏幕上完成点餐的操作。它还能执行一些固定的基本流程，我觉得更复杂的操作很快也能实现，因为确实很多人在做这方面的开发和优化。再比如在智能眼镜、智能耳机里，其实已经可以做一些定向的视觉识别和语音辅助功能了。

简单来说，你用眼镜对着一段文字，眼镜里可以弹出翻译后的文字提示——这种基本的文字识别和翻译，现在技术上已经是可行的。但是，如果你希望眼镜所见的一切，比如整段文字都实时替换成另一种语言在原环境中呈现（比如把博物馆展牌上的法语直接替换成中文显示在你眼中），这里面还存在很大的难题。涉及到图像中文字的渲染、Diffusion 生成这类技术，特别是对于一些非拉丁字母的小语种文字，生成效果目前还不够理想。我之前看过一些最新的尝试，生成的替换文字常常奇形怪状，像一堆小虫子在爬，远没有达到可以实用的程度。

不过，我认为这可能在一两年内会出现突破。毕竟这些问题很多人都在研究，一旦有方法可行，应用层面推进会很快。另外，像您提到的耳机同传（同声传译），其实现在相关技术相对成熟一些。对于大部分主流语言之间，我们已经可以做到语音输入 -> 实时翻译 -> 目标语言语音输出，还有文字转写等功能。只是如果涉及一些非常小众的语言，把语音翻译成文字显示，可能效果还受限于语言的数据量不足。这又回到多语言的老问题了，小语种缺少数据，模型就很难训好。这部分体验暂时还有提升空间。

**唐小引**：刚刚我们聊的是把模型塞进各种智能设备里。我还蛮好奇系统级整合这个话题。现在大家都很期待手机、电脑的厂商能不能把大模型直接集成进操作系统层面。毕竟目前我们用手机上的 AI 模型，大多是装各种 App（比如通义、ChatGPT、Gemini 等），用起来和系统原生体验区别挺大，模型和操作系统的结合现在进展到什么程度了？

**杨宝嵩**：目前来看，应该说大部分功能在技术上都可以集成到系统里，但是有一些过于开放的能力，现在还不敢放到系统层面去。一是因为大模型偶尔还有幻觉、错误，做不到百分之百可靠。把这样不确定性的东西深度集成，很可能出现不可控的问题。但我认为要视场景而定：绝大多数交互场景，其实并不需要 100% 准确率。相比以前的 Siri，现在大模型给你的响应即使偶尔不完美，但在理解你的意图、陪伴交流上已经是巨大的飞跃了。所以语音助手这一块的发展非常快。接下来真正难点是那些需要强确定性和安全性的操作，这部分可能短时间内还是得保守一点。

**唐小引**：是的，比如现在大家讨论的端侧 AI 主要集中在手机和汽车两个场景。手机上更多是数据隐私和安全的问题，但汽车上涉及人身安全，您认为车载结合大模型和现在很热的 Agent 技术，会带来什么样的变化？以及在安全层面，该如何看待？

**杨宝嵩**：我先声明，我不是直接做车载 AI 的，这方面只是个人观点。我认为汽车里有非常多地方可以应用 AI，但不代表所有部分都要上大模型。像车内的语音助手，开关窗、调节空调这些指令，用大模型来做其实挺方便的，用户体验会比传统固定命令好很多。再比如车载导航、出行规划这块，其实也可以引入大模型来增强体验。举个场景：假设我对车里的 AI 说“我现在在巴黎，待会想去吃午饭，然后下午随便逛逛，你给我推荐几个不错的餐馆和景点并规划路线”。传统导航软件只能一个功能一个功能来，你得自己搜索、看评价、再设导航。有了大模型代理， 它可以帮你把这些链条式的任务都串起来：先在后台调用点评类的服务搜附近餐厅，筛选高评分又符合你口味的；选好餐厅后再调用导航服务自动帮你设好路线。

整个过程，大模型就像一个调度者或者高级助手，真正执行导航的还是原来的导航软件，但用户这边体验就是一句话搞定。我相信这种场景下，大模型未来大有用武之地——它擅长理解你的复杂意图，擅长在不同工具和服务之间做决策、做操作。至于您提到的安全问题，比如自动驾驶那种和人身安全强相关的，我个人觉得短期内大模型不会贸然直接介入。自动驾驶涉及的传感器处理、控制系统那些，有各自成熟的方案，不一定要用大语言模型来做。大模型更适合做人机交互、辅助决策这一块。所以安全方面，至少交互层面的安全是可以通过不断改进模型、设置防御策略来保障的；而驾驶控制层面的安全，我估计业界还是会谨慎，让大模型该做决策时做决策，该交回专业系统时就交回去。

**唐小引**：现在看来，像您说的娱乐信息和交互需求上用大模型没问题，但真正涉及车辆驾驶决策，大多数人还是持保留态度。这部分我们也拭目以待后续行业怎么发展。

**杨宝嵩**：是的，安全永远是重中之重。

**唐小引**：最后我想问个总结性的问题。从您个人经历来看，您是从机器翻译一路做到现在大模型的多语言能力研发，可以说见证了 AI 在语言领域的几次阶段性跨越。现在很多人讨论“大模型会不会取代 XX”的话题。我想请您谈谈“取代”这件事的好与不好。比如以翻译为例，现在我们已经能用 AI 实现相当不错的机器翻译和同传，那么对于人工翻译从业者来说是危机，对行业来说又是机遇，对大众则是福利。您怎么看待这种技术替代带来的机遇和挑战？

**杨宝嵩**：我分两方面来说。首先，就机器翻译这个领域本身来说，大模型的出现并没有替代机器翻译，它只是成为了机器翻译里面的一种新技术路线，就像当年统计机器翻译被神经机器翻译替代一样，现在神经网络又升级成大模型来做翻译。本质上讲，机器翻译是一个长期的需求和问题领域，而大模型的引入让我们能够探索更多可能性，解决以前解决不了的问题。比如覆盖更多的小语种，支持图像、视频这类多模态的翻译，甚至做到实时的语音同传等等。这些新能力拓展了机器翻译的边界，也使得这个行业会发展的更好、更有价值。所以我不觉得大模型是毁掉了机器翻译行业，相反，它给这个方向注入了新活力，最终会造福更多的人。

至于对人工语言工作者（译员、同传等）的影响，我觉得至少目前为止，可以把大模型视作效率工具，就像程序员看待代码自动补全、代码生成工具那样。我们为什么一定要每天苦干十个小时呢？能不能像法国人那样，下午三点就去悠闲遛狗？我的意思是，如果 AI 工具能提高我们的生产力，我们就应该拥抱它，把重复繁重的部分交给 AI，自己则提升技能去做更高价值的事情。当然，这需要整个社会和用人单位观念的转变。如果只是单方面要求从业者又要用 AI 又要加量不加价，那肯定会引起焦虑和不公平。不过长远看，我相信优秀的人工翻译、作家这些创意和语言领域的人才，反而更不容易被取代。因为这些工作产出的评价标准是“好不好”，而不是简单的“对或错”。代码不对就是不对，AI 把它写对了那程序员的价值确实被取代了一部分。但翻译、写作不太一样，有时候 AI 翻译出来是正确但平淡，而人类翻译能够融入更多巧思、文采，那客户还是会选人类的作品。所以真正顶尖的语言专家我认为更容易保住饭碗，甚至比程序员更安全。

**唐小引**：您这一番话本来是想安慰一下大众，让大家别过度焦虑，结果 CSDN 上的程序员们听完可能更焦虑了。

**杨宝嵩**：哈哈，其实也不用焦虑。我觉得未来肯定也会出现新的职位和机会。就像我刚才举的例子，以后可能会有一种新的程序员，叫 AI 编程师或者 Prompt 工程师之类的，专门负责和各种 AI 打交道，帮 AI 更好地完成任务。这有点类似现在大家说的各种 Agent 调度。最近很热的 MCP 协议其实就是让不同模型、工具协同工作。未来懂得把不同 AI 工具组合起来解决复杂问题的人，会非常抢手。这其实也是一种新的开发工作，只不过对象从传统的代码逻辑变成了如何编排多个 AI 协同。

**唐小引**：说到这点，确实像您提到的，现在已经有一些 AI 编程/AI Agent 平台在做类似的事了。例如 Cursor 等开发工具，其实就可以直接调用各种模型和插件，帮开发者完成复杂任务的编排。国内很多团队也在钻研这一块。所以您说的“新程序员”这个角色，其实现在已经在雏形阶段了。

**杨宝嵩**：是的，本质上和当前的探索是一样的。我想强调的是，随着 AI 能力在各行各业、各类设备中铺开，这方面的人才需求只会越来越多。当智能手机、汽车、眼镜等等底层的 AI 接口和协议都成熟完善后，在上面开发各种 AI 应用就会变得非常容易且繁荣。那时候所谓的“AI 应用开发工程师”估计比现在的移动开发、Web 开发还要常见，这个领域大有可为。

**唐小引**：说得很好。节目最后，我们再把目光拉回您最擅长的多语言 AI 这一块。对于您，以及所有致力于多语言大模型研究的从业者来说，目前已经取得了哪些成果？接下来还希望突破的核心难题是什么？也请您展望一下未来要重点攻克的方向。

**杨宝嵩**：当前大模型在多语言方面的整体进展，相对慢于比如数学、编程这些热点方向。在行业讨论热度上，多语言没有那么风口浪尖，但它其实又非常重要。以我们千问为例，现在可以做到的阶段性成果是：对于那些高资源语种（数据足够丰富的语言），模型基本能理解指令，进行比较流畅的对话和回答。这算是实现了一定程度的“多语言通用能力”。

但是有两大问题我觉得目前还没有解决，甚至短期内都不一定能完全解决：首先是不同文化的对齐问题。刚才咱们也提到，比如同样问“晚上吃什么”，中国用户期望的回答和阿拉伯用户期望的可能完全不同——前者可能期待一些本地美食建议，后者还得考虑清真不清真、宗教禁忌等等。未来作为一个 AI 助手，要在角色扮演、情感陪护以及各种场景下都满足不同文化用户的需求，这对模型来说很难。我们最近做了一些评测也能发现，现在很多大模型体现出的“文化”和“价值观”要么就是很东方化，要么就是很英美化（盎格鲁-撒克逊化）。这其实说明模型在文化多样性上还有很大提升空间。

**唐小引**：您觉得这个问题可以解决吗？

**杨宝嵩**：相对来说是能解决的。可以通过一些强化学习和价值观对齐的方式去调整模型在不同文化下的表现。当然这里面有一个更深层的难点是知识覆盖。也就是说模型脑子里对不同文化背景的知识储备。这可能需要通过扩大数据规模、提升模型参数，或者接入 RAG（Retrieval Augmented Generation，即检索增强生成）等手段来补充。

但目前来看，多文化对齐还不算最棘手，真正不太好解决的是我接下来要说的第二点——各语言的生成能力。简单说，就是让模型说不同语言的时候是不是够自然、够像人。在一些大语种上（比如中英文），大家可能觉得现在大模型输出已经挺像人了，但其实仔细一用还是会发现有时候不太地道或者风格怪怪的，更别提那些小语种了。全世界有上千种语言，而目前大模型真正能比较流畅地支持生成和翻译的可能也就几十种，占比不到 1%。绝大多数语言，大模型最多能做到“听得懂”，但很难“说得好”。要让模型用每一种小语种都写出像模像样的段落，这条路还很长。

而且说实话，这个问题很难解决。因为根据我们的研究和经验，生成这件事情非常依赖 Scaling，也就是依赖海量的数据和超大的模型。但偏偏在多语言上，Scaling 遇到了严峻挑战——很多语言它就是没有那么多数据给你。所以未来我想一定要靠合成数据这条路，或者甚至跨模态的迁移，把其它模态（比如语音、图像里的信息）转化为文本数据来丰富小语种。这方面我们也在积极思考和尝试。

**唐小引：这些是您眼前就已经顾得上的“头疼问题”吗，还是说觉得它更像是长远的战略难题？**

**杨宝嵩**：既是长远的，也是眼前正在着手做的。我们作为研究人员，一方面要有长期攻关的意识，另一方面遇到问题该解决还是得解决，不能因为难就不管。目前这些方向我们团队都有投入，人手再紧也得硬上。

但这也正是科研的有趣之处，有挑战才有动力。除了刚才说的两个，我认为还有第三个大问题，就是多语言和多模态的融合。这块现在业内研究的还比较少。比如，让模型识别图片里的多语言文字、或者生成多语言的语音和字幕，甚至对于不同口音、方言的理解和生成，这些都是才刚起步。我相信以后图像、语音和文本会越来越结合，而语言又有这么多种类，这里面肯定还有很多文章可做。

**唐小引**：听起来您是在呼唤更多的研究者赶紧加入一起攻克这些难题啊！

**杨宝嵩**：确实是希望有更多人一起努力。在千问项目上，我们选择把模型开源出来，提供一个高性能的基础模型，目的也是希望整个开源社区能够和我们一道把事情做好。其实我们自己也从开源社区受益良多。比如一些优秀的数据集项目像 FineWeb、CulturaX，都是社区贡献的，让我们能获取到更高质量的多语言训练数据；还有各种各样的 Benchmark 和数据过滤技术，这些都是社区智慧的结晶。

可以说，只有大家一起共建，这些多语言长期难题才能解决得更好。我也不觉得光靠我们千问团队，或者光靠某个巨头的闭门团队，就能把 AGI 实现了。这一定需要成百上千、乃至上万的科学家和开发者同心协力，才可能最终达成目标。就拿我前面提到的多语言数据问题来说，比如 FineWeb 2.0 那个项目，他们为了清洗互联网数据，就每种语言都找母语专家或众包人员去制定过滤规则，甚至训练专门的分类模型来过滤数据。这种资源和投入对我们一个团队来说太难了，但靠社区的力量就能完成。这正是开源共建的价值所在。

**唐小引**：这就是开源的魅力！让我们一起 Enjoy 开源，Enjoy AI 吧！非常感谢杨老师今天带来如此深入而精彩的分享！也感谢各位观众的观看。欢迎大家持续关注 GOSIM 的 X、YouTube、B 站等账号，我们将不断为大家奉上更多来自开源与 AI 前沿的技术分享和观点见解。本期节目就到这里，我们下次再见啦。

