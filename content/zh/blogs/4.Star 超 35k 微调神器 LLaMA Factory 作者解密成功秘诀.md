---
title: Star 超 35k 微调神器 LLaMA Factory 作者解密成功秘诀
date: 24 June 2025
description: '大模型训练和开源 LLaMA Factory 背后的故事、思考和实践经验。 '
image: /blogs-img/blog4-郑耀威.jpg
alt: First Post
ogImage: /blogs-img/blog4-郑耀威.jpg
tags: ['gosim', 'open-source', 'grassroots']
published: true
---



*作者 | 何苗*
*采访 | 唐小引*
*出品 | GOSIM 开源创新汇*

2024 年，LLaMA Factory 横空出世，迅速在开源社区引发轰动，短短一年左右便收获 35k Star，成为国内外开发者眼中的“微调神器”。这个由北京航空航天大学研究员郑耀威主导的项目，不仅让大模型的微调变得前所未有的简单高效，更通过开源的力量，汇聚了全球开发者的智慧与热情。

郑耀威，这位年轻的开源爱好者和实践者，从大学时期就开始与AI结缘，一路见证了人工智能从实验室走向大众视野的全过程。他曾在开源社区中一路成长，从理论到实践，从代码到社区协作，愈加深刻地理解开源的力量。如今，他带着 LLaMA Factory，开启了大模型微调的新篇章。

由 GOSIM 联合 CSDN 重磅打造的 Open AGI Forum 栏目。在 GOSIM CHINA 2024 的现场，很荣幸的邀请到了北京航空航天大学研究员、LLaMA Factory 项目的作者郑耀威，与我们深入分享他的技术生涯，以及在大模型训练和开源 LLaMA Factory 背后的故事、思考和实践经验。 

**精彩观点抢先看：**

* 一开始我对开源的理解比较理论化，但参与开源项目的维护和发展之后，处理众多的反馈和进行代码审查，让我看到了协作的力量和快速解决问题的能力，还能接触到全球不同背景贡献者的创新思路。

* 在构建和维护复杂系统时，开发者必须深入了解第三方库的内部机制，以确保整个系统的稳定性和性能。这种依赖关系使得理解和维护第三方库成为项目成功的关键因素之一。 

* AI 编程助手并不是 LLaMA Factory 主体微调的方向。对于这类需求，可以采用多个模型协同工作的方式来实现。

* 微调不仅是改进模型性能的关键步骤，也是探索其局限性和潜在问题的重要途径。

* 我们通常会选择训练多个小模型来协同工作，每个小模型专注于独立承担一项特定任务，通过这种方式，我们不仅能够提高特定任务的性能，还能优化整个工程系统的效率和效果。

* 在实践中，可能会花 50%的时间构建数据，20%的时间进行微调，30%的时间用于评估。我们开发 LLaMA Factory 目的就是为了让微调过程更加高效。


##  “我的开源与 AI 之路并行发展” 

**唐小引：你是如何跟人工智能结缘的？**

**郑耀威：**2017 年，我刚踏入大学校门，那时的人工智能领域远不如今天这般炙手可热。初到北京这座大都市，我开始接触到人工智能的知识是因为有幸参与了多场相关会议和讲座，不仅限于北京航空航天大学，还包括清华大学和北京大学等举办的研讨会。在那些活动中，我初次感受到了前沿技术的魅力，对人工智能产生了浓厚的兴趣。进入大二后，在兼顾学业的同时深入学习了人工智能技术。那时候虽然还没有出现大模型的概念，但我已经开始了神经网络的学习，并进行了一些基础实验。这些早期的经历累积起了我对人工智能领域的认知与热情。我发现还是需要一些引路人，因此后来加入了学校的实验室，在老师的带领下去做了一些探索。 

**唐小引：谁是你进入大模型、走进人工智能大门的引路人？**
 
**郑耀威：**刚进入大学不久，我就参加了一场的讲座，可以说对我影响深远。这场讲座由一位从海外留学归来的校友主讲，他分享了关于机器学习与隐私保护的研究成果。他不仅在母校留下了重要的学术足迹，在美国也有丰富的教学经历。讲座的内容对当时的我来说有些晦涩难懂，但它成为了我学术旅程的重要起点。
 
**唐小引：对开源的了解始于何时？** 

**郑耀威：**我的开源之路与 AI 之路是并行发展的。在逐渐接触开源社区的过程中，我的思考和工作方式产生了一定的改变。例如，社区中用户提出的大量 issue 和各种问题，以及开发者们提交的 PR，这些实际互动所带来的体验很有意思。 一开始我对开源的理解可能比较理论化，但参与开源项目的维护和发展之后，处理众多的反馈和进行代码审查，让我看到了协作的力量和快速解决问题的能力，还能接触到全球不同背景贡献者的创新思路。这让我认识到，开源社区充满了多样性和无限的可能性。 在开展 LLaMA Factory 之前，我接触过 GitHub 上非常优秀的开源项目。

**唐小引：LLAMA Factory 在启动之初就选择了开源的道路，你对开源的理解是怎样的？什么原因促使你在项目开始时就决定走这条开源之路？**

**郑耀威：**我目前在北航读博，并参与实验室的大模型相关研究。去年 4 月，我们注意到像 LLaMA 这样的开源大模型发布后，开源大模型的微调一定会成为将来一个热门的方向。因此很早就开始构建大模型微调框架，经过持续的维护和改进，LLaMA Factory 框架现已发展成为国内外广受欢迎的大模型微调工具。这一框架在国内和国际社区中都获得了显著的关注和支持。 

其实 LLaMA Factory 是我第一个做开源的项目，之前我们会把一些科研代码去开放出来给大家去复现，但这可能不是真正的开源，一般来说科研的论文代码在发布之后就不再会维护了，但是 LLaMA Factory 开源项目不一样，它需要经过持续的这个维护迭代才可以获得如此大的社区用户。 

## LLaMA Factory 的诞生与挑战 

**唐小引：****短时间内 LLaMA Factory 就在开源社区中取得了令人瞩目的成绩，影响力非常广泛。截至当前，该项目已经获得了超 35k 的 Star。你是如何将它推广得如此成功的？**

**郑耀威：**开源项目最吸引人的地方往往在于其创始人的独特创意。为了在社区中引起关注，创始人通常会提出一个新颖且具有吸引力的想法。当这个想法足够创新和稀有时，它就有可能吸引广泛的注意，从而让开源项目迅速走红。我们创建 LLaMA Factory 的初衷正是基于这样的理念。我们的目标是使大规模模型的微调变得更加普及，让任何人都能够轻松上手进行大模型微调。因此，LLaMA Factory 的一个重要使命就是简化大模型微调的过程，尽可能降低门槛，使得更多的人可以快速接触并利用这一技术。我们希望让更多人受益于这项技术。 

**唐小引： LLaMA Factory 团队现在有多少人？** 

**郑耀威：**目前我们的团队规模保持在精简状态，便于高效管理和运作。团队核心成员包括大约 5-6 位研究生，共同致力于框架的维护与开发。除此之外，还得到了来自开源社区的广泛支持和贡献，尤其是国内的贡献者非常活跃。许多程序员和开发者从全国各地积极参与到项目中来，为项目的成长注入了强大的动力。 

**唐小引：做 LLaMA Factory 至今遇到过什么样的坑？**

**郑耀威**：做开源项目必然会遇到的一个坑，就是第三方库引入的一些 bug，这其中包括了第三方库自身的 bug 或由于版本更新导致的兼容性问题。我们所使用的框架深度依赖于多个知名的开源库，例如 PyTouch、DeepSpeed 以及 Transformers。 

当前大模型及整个人工智能系统已经变得极其复杂和庞大，单靠自身代码几乎无法完成从头构建的任务，为了确保系统的高效运行和开发流程的顺畅，必须依赖第三方基础设施和库的支持。这意味着大量的底层工作是由这些第三方库来完成的，而我们在上层进行开发时，不仅需要深刻理解这些库的工作原理，还要确保它们能够稳定运行并进行必要的维护。在构建和维护复杂系统时，开发者必须深入了解第三方库的内部机制，以确保整个系统的稳定性和性能。这种依赖关系使得理解和维护第三方库成为项目成功的关键因素之一。 

尽管这些开源库持续更新，但过程中难免会引入新的 bug。我们在开发过程中确实遇到了这些问题，并在使用这些库的过程中积极地向社区反馈。在使用 Hugging Face 的 Transformer 库时，我们提交了近 10 个 PR，帮助修复发现的 bug。 

**唐小引：你有看过与 LLaMA Factory 类似的开源项目或者闭源项目吗？核心的差异特征有哪些？**

**郑耀威：**在对比国外项目时，Hugging Face 的 AutoTrain这类项目确实做得比较完善，并集成了大量的微调算法。然而，我认为最核心的差异化还是在用户体验的层次上。国外的极客们往往更关注技术更新，在用户体验方面可能投入的精力相对较少，而我们因为社区规模较大，所以希望能够投入更多的人力和资源来维护软件的使用方式，包括安装、应用性、启动以及 bug 修复等方面。 


## 微调神器关键词效率、效率！

**唐小引：微调神器 LLaMA Factory 在微调这一块，现在有什么变化吗？**

**郑耀威：**新的模型模态的引入为微调领域带来了新的机会。过去，我们仅能对文本大语言模型进行微调，使其具备生成文字的能力。然而，随着技术的发展，OpenAI 发布了 GPT-4o，开源社区涌现出许多的多模态大模型，我们现在可以借助这些模型的多模态能力，将视觉和音频等新模态的任务引入到微调过程中，从而开展更多创新性的尝试。在数据领域，尤其是在自然界的数据集里，并非所有数据都是文本单模态的，它们往往包含了丰富的视觉信息。例如，摄像头所获取的数据、科学研究中的观测记录等，通常是以视觉信息作为主要输入形式。我们可以利用多模态大模型，探索更多应用场景的可能性。 

**唐小引：LLaMA Factory 如何支持多种多样的模型的微调？模型的多样性、复杂性怎么从技术层面去确保？** 

**郑耀威：**多种模型的接入是个非常大的挑战，因为不同模型对于数据的要求以及整个流水线的要求都不相同。在开发过程中，我们尽可能将数据和模型这两个层面解耦开，也就是说，我们将数据流水线统一化，希望最终可以提供一个模型的统一出入口来完成模型推理。只要模型推理成功后，其微调就不那么困难了，因为它只需要获取到模型参数并完成相关操作。 

**唐小引：模型微调同样适用于特定的技术场景，可以通过一个具体的代码场景案例，展示如何利用 LLaMA Factory 进行微调吗？**

**郑耀威：**实际上，代码层面的应用确实比较复杂，可以分为多个类别。如果只是让模型生成一些简单的函数，例如编写快速排序算法（快排），这种任务并不需要微调。如果我们希望模型能够生成具有独特属性的程序，例如为一个庞大的新项目生成测试案例或补充特定功能，这就需要模型学习该项目本身的知识。在这种情况下，我们需要将项目的代码整理成适合微调的数据集。通过这种方式，微调后的模型能够了解项目本身的架构和功能，从而更好地完成任务。 

以编写算子为例，我们可以将 CUDA 代码和内核代码提供给大模型，让其学习并尝试生成高效的算子。这种情况下，我们需要将项目的代码整理成适合微调的数据集，通过这种方式，微调后的模型能够了解项目本身的架构和功能，从而更好地完成任务。 我们也不希望模型一次学太多东西，而是在某一个方面成为一个专家。 
AI 编程助手并不是 LLaMA Factory 主体用来微调的方向。对于这类需求，可以采用多个模型协同工作的方式来实现。也就是说，我们可以构建一个由众多专门模型组成的系统，每个模型在其特定的领域内都是专家。这样，整个系统就如同一个混合专家系统，各个模型共同协作来完成同一任务，每个方面都由对应的专家模型负责处理。 

**唐小引：对一个公司来讲，需要配备自己的算法团队吗？**

**郑耀威：**我认为大模型微调不应仅由个别人完成，而应该是更多人能够接触和参与的过程。通过微调能更清晰地理解大模型的工作机制。大模型并非无所不能的，实际上，它们基于神经网络，其性能会受到数据的影响。在微调过程中，我们可以发现哪些边缘案例或压力条件会影响模型的输出，从而导致异常结果。通过这些调整，不仅能优化模型的表现，还能深入了解它对不同类型数据的响应方式。因此，微调不仅是改进模型性能的关键步骤，也是探索其局限性和潜在问题的重要途径。让尽可能多的人参与到大模型微调中来，可以帮助我们更全面地评估和理解这些模型的能力与限制，进而推动整个领域的进步。


## LLaMA Factory 的用户与市场潜力

**唐小引：LLaMA Factory 当前贡献者里面，有多少来自于中国？多少来自于海外，整个用户构成是怎么样的？ **

**郑耀威：**LLaMA Factory 在用户层面上，在国内和国外都有一定的用户群体。但在贡献者层面上，国内的贡献者相对较多。这主要是因为我们一直维护着社群，交流较为频繁，因此吸引了不少国内开发者将他们的想法贡献到框架中。这些贡献者可能来自企业或高校的研究生成者。有一些小型企业在创业阶段会重度使用这个框架去支撑他们的项目，积极贡献一些功能。 

**唐小引：看起来 LLaMA Factory 更多的应用场景是与企业侧的具体场景相结合，它如何与这些具体场景结合起来？**

**郑耀威：**模型微调本身必然要与一些垂直领域的场景适配。如果只是让模型进行通用问答，其实不需要微调，直接使用原始模型即可。如果希望模型学习新的知识，通常就需要涉及到微调的场景。例如，在医学、法律等专业领域，以及更垂直的领域如航空和消防等，这些特别专业的领域需要通过微调来让模型适应特定的需求和知识体系。 

**唐小引：可以具体展开一下在某一个行业里的落地案例吗？**

**郑耀威：**尽管我们无法跟踪用户在具体场景中的使用情况，但有些用户会主动给我们反馈他们的成功案例。例如，一家名为筷子科技的公司，他们基于 LLaMA Factory 在短视频领域进行了一次大模型实践，开发了一个视频应用，效果非常显著。特别值得一提的是，他们在微调之后发现，一个 7B 参数的模型就能取得优异的表现，甚至优于未微调的更大模型。这证明了 LLaMA Factory 的有效性和灵活性，也展示了微调技术在特定应用场景下的强大潜力。 


## 未来趋势：小模型 or 大模型？

**唐小引：**用小模型完成更复杂任务是否会成为未来大模型发展的一个方向？这是一个值得探讨的问题。在我个人的经历中，可以明显感觉到，在 ChatGPT 出现之前，AI 圈子内已经掀起了一股追求构建大模型的热潮。随着 GPT 系列模型的推出，这一趋势达到了顶峰，大模型因其强大的泛化能力和广泛的应用潜力而备受青睐。 

然而，随着技术的发展和实际应用场景的需求变化，人们开始意识到小模型也有其独特的优势。特别是在资源受限的环境中，如特定垂直领域、端侧设备以及边缘计算场景中，小模型因其高效性和低功耗特性而受到欢迎。许多人希望利用通用的小模型来处理各种任务，类似于编程中“一次编写，到处运行”的理念。 

**我们可以观察到一种趋势的转变：从对大模型的狂热追求转向对小模型的关注。这种转变并非否定大模型的价值，而是反映了 AI 社区对于不同模型尺寸在不同应用场景中的平衡考量。面对这种起伏变化，我的困惑在于如何理解这些变化背后的逻辑，以及它们对未来 AI 发展方向的影响。你的思考和观察是怎样的？**

**郑耀威：**与 2020 年相比，最明显的区别在于小模型在 2023 年开始变得尤为突出。例如，国外的 Meta 以及 Mistral 昨天刚发布的小模型，都展示了这一趋势。在国内，像面壁智能这样的公司也一直在专注于端侧模型的发展。这表明，尽管大模型依然重要，但小模型因其特定优势而在不同应用场景中获得了更多的关注和发展机会。 

**唐小引：在实际的行业落地过程中，我们常常发现大模型并不能很好地解决特定问题。在实践中，你是否也遇到了类似的挑战，并找到了应对之道？**

**郑耀威：**实际上，我们都有过亲身体验：大模型虽然在所有任务上都能达到大约 70%的准确率，但这是在广泛任务上的平均表现。相比之下，如果我们使用小模型，可以发现它在特定任务上的准确率能够达到 90%以上。假设我们拥有多个这样的小模型，每个都在各自的任务上达到 90%以上的准确率，那么这些小模型整体的表现将远远超过那个单一的大模型。 因此，在实践中，我们通常会选择训练多个小模型来协同工作，每个小模型专注于独立承担一项特定任务，而不是试图用一个大模型来承担所有任务。通过这种方式，我们不仅能够提高特定任务的性能，还能优化整个工程系统的效率和效果。这种方法使得我们可以更精准地应对不同任务的需求，从而实现更高效的解决方案。
 
**唐小引：基于现在 LLaMA Factory 的这个发展情况，有考虑过创业的事情吗？**

**郑耀威：**创业确实是一个风险较大的事情。我们需要找到用户，并解决他们本地资源部署的问题，这些都非常耗费人力。因此，在现阶段，我认为应该先把开源项目做得更大，确保软件可以推广到更多的用户群体中。在达成这一目标后再考虑其他的扩展和发展。 去年一直在深入思考的问题：是否能够真正找到一个通用的解决方案，来完成所有任务？经过一番考量后，我们意识到当前的软件工具，例如 LLaMA Factory 以及推理框架等，在大模型落地过程中实际上扮演的是中间件的角色，它们只是整个流程的一部分。 

大模型的实际应用面临的一个较大挑战在于，从数据准备到模型评估，整个落地环节非常复杂。这些现有的工具和框架虽然在某些方面提供了支持，但要实现大模型的成功部署，还需要前后端诸多环节的配合，包括数据处理、模型训练、性能评估等多个步骤。因此，目前我们仍在等待一个更加全面和通用的解决方案，以真正打通大模型从开发到实际应用的全流程。这意味着，尽管现有的工具已经为大模型的应用提供了一定程度的支持，但我们仍需继续探索和完善，以构建一个能够覆盖从数据准备到最终部署所有环节的通用解决方案，才可以真的去完成这个落地的事件。

**唐小引：未来模型的趋势会发生什么样的变化?** 

**郑耀威：**在未来两年内，我特别关注的领域是大模型如何更深入地融入人们的日常生活中，发挥其实用效能，而不仅仅是停留在当前的问答助手或服务于高端用户的应用上。我希望看到大模型逐渐在更基础的层面实现落地，例如在日常生活管理和家庭助手方面提供帮助。我相信，在接下来的一两年内，我们会见证大模型在这方面的显著进展。 

**唐小引：从开源到微调这两方面有哪些经验和建议可以给开发者朋友们分享？**

**郑耀威：**从开源项目的角度出发，我认为做开源项目最重要的是有一个贴近实际需求的好想法。这个想法应源于身边的痛点，而不是远离自身的生活或工作领域。比如，我们从实验室的需求出发，关注低资源或低成本的大模型微调，启动了这个项目。好的开源项目应该是切实可行且易于入手的。

关于创新性，最初的想法不必过于新颖，随着项目的推进，通过倾听社区的反馈，我们可以不断改进和增加创新点。这些创新往往来自于用户在实际使用过程中遇到的问题和建议。通过解决这些问题，我们逐步完善了 LLaMA Factory 框架，使其更加实用。 

虽然大模型微调非常重要，但数据准备和评估同样关键。在实践中，我们可能会花 50%的时间构建数据，20%的时间进行微调，30%的时间用于评估。我们开发此工具的目的就是为了让微调过程更加高效，只需花费 20%的时间，从而留出更多时间和精力专注于数据构建和评估系统的优化。

**唐小引：**感谢郑博的分享，谢谢大家的关注，我们下次再见。

